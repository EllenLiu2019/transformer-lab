{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758230e4",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 多头注意力\n",
    ":label:`sec_multihead-attention`\n",
    "\n",
    "In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.\n",
    "\n",
    "实际上，对于相同的查询集、键集和值集，我们可能希望模型能够结合同一注意力机制的不同行为所蕴含的知识，例如在一个序列中捕捉不同范围（例如短范围与长范围）的依赖关系。因此，允许注意力机制联合使用查询、键和值的不同表示子空间可能是有益的。\n",
    "\n",
    "To this end, instead of performing a single attention pooling, queries, keys, and values can be transformed with $h$ independently learned linear projections. Then these $h$ projected queries, keys, and values are fed into attention pooling in parallel. In the end, $h$ attention-pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention, where each of the $h$ attention pooling outputs is a head (Vaswani et al., 2017). Using fully connected layers to perform learnable linear transformations, Fig. 11.5.1 describes multi-head attention.\n",
    "\n",
    "为此，可以使用 $h$ 个独立学习的线性投影分别对查询、键和值进行转换，而不是执行单次注意力池化。然后将这 $h$ 个投影后的查询、键和值并行输入注意力池化。最后，将 $h$ 个注意力池化的输出进行拼接，并通过另一个学习的线性投影转换以生成最终输出。这种设计称为多头注意力，其中每个注意力池化的输出都是一个头（Vaswani 等人，2017）。图 11.5.1 使用全连接层执行可学习的线性变换来描述多头注意力。\n",
    "\n",
    "![多头注意力：多个头连结然后线性变换](../images/multi-head-attention.png)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型\n",
    "\n",
    "在实现多头注意力之前，让我们用数学语言将这个模型形式化地描述出来。\n",
    "给定查询 $\\mathbf{q} \\in \\mathbb{R}^{d_q}$ 、\n",
    "键 $\\mathbf{k} \\in \\mathbb{R}^{d_k}$ 和\n",
    "值 $\\mathbf{v} \\in \\mathbb{R}^{d_v}$，\n",
    "每个注意力头 $\\mathbf{h}_i$（ $i = 1, \\ldots, h$ ）的计算方法为：\n",
    "\n",
    "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n",
    "\n",
    "其中，可学习的参数包括\n",
    "$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$、\n",
    "$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$和\n",
    "$\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$，\n",
    "以及代表 `注意力汇聚的函数(attention pooling)` $f$。\n",
    "$f$ 可以是 `additive attention` 或者 `scaled dot product attention`\n",
    "多头注意力的输出需要经过另一个线性转换，它对应着 $h$ 个头连结后的结果，因此其可学习参数是 $\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$ ：\n",
    "\n",
    "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n",
    "\n",
    "Based on this design, each head may attend to different parts of the input. More sophisticated functions than the simple weighted average can be expressed.\n",
    "\n",
    "基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。"
   ],
   "id": "37d9e6fdc6fec15f"
  },
  {
   "cell_type": "code",
   "id": "3f43e730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:46.337932Z",
     "iopub.status.busy": "2023-08-18T07:00:46.337257Z",
     "iopub.status.idle": "2023-08-18T07:00:49.061240Z",
     "shell.execute_reply": "2023-08-18T07:00:49.060247Z"
    },
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-15T05:44:21.803518Z",
     "start_time": "2025-10-15T05:44:19.675134Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c564c606",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## 实现\n",
    "\n",
    "在实现过程中通常[**选择缩放点积注意力作为每一个注意力头**]。\n",
    "为了避免计算代价和参数代价的大幅增长，\n",
    "我们设定 $p_q = p_k = p_v = p_o / h$。\n",
    "值得注意的是，如果将查询、键和值的线性变换的输出数量设置为\n",
    "$p_q h = p_k h = p_v h = p_o$，\n",
    "则可以并行计算 $h$ 个头。\n",
    "在下面的实现中，$p_o$ 是通过参数 `num_hiddens` 指定的。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "40d3817b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:49.066203Z",
     "iopub.status.busy": "2023-08-18T07:00:49.065456Z",
     "iopub.status.idle": "2023-08-18T07:00:49.075837Z",
     "shell.execute_reply": "2023-08-18T07:00:49.074932Z"
    },
    "origin_pos": 6,
    "tab": [
     "mxnet"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-15T07:36:58.953777Z",
     "start_time": "2025-10-15T07:36:58.919140Z"
    }
   },
   "source": [
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，查询的个数， num_hiddens/num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "e9c740c8",
   "metadata": {
    "origin_pos": 10
   },
   "source": "为了能够 **使多个头并行计算**，上面的 `MultiHeadAttention` 类将使用下面定义的两个转置函数。具体来说，`transpose_output` 函数反转了 `transpose_qkv` 函数的操作。\n"
  },
  {
   "cell_type": "code",
   "id": "98a99844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:49.080064Z",
     "iopub.status.busy": "2023-08-18T07:00:49.079441Z",
     "iopub.status.idle": "2023-08-18T07:00:49.086286Z",
     "shell.execute_reply": "2023-08-18T07:00:49.085363Z"
    },
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-15T07:37:01.068839Z",
     "start_time": "2025-10-15T07:37:01.064365Z"
    }
   },
   "source": [
    "#@save\n",
    "@d2l.add_to_class(MultiHeadAttention)\n",
    "def transpose_qkv(self, X):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads, 查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "#@save\n",
    "@d2l.add_to_class(MultiHeadAttention)\n",
    "def transpose_output(self, X):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "3277f135",
   "metadata": {
    "origin_pos": 15
   },
   "source": "下面使用键和值相同的小例子来[**测试**]我们编写的`MultiHeadAttention`类。多头注意力输出的形状是（`batch_size`，`num_queries`，`num_hiddens`）。"
  },
  {
   "cell_type": "code",
   "id": "5e8a81a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:49.090585Z",
     "iopub.status.busy": "2023-08-18T07:00:49.089798Z",
     "iopub.status.idle": "2023-08-18T07:00:49.094758Z",
     "shell.execute_reply": "2023-08-18T07:00:49.093923Z"
    },
    "origin_pos": 16,
    "tab": [
     "mxnet"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-15T07:37:04.858332Z",
     "start_time": "2025-10-15T07:37:04.849312Z"
    }
   },
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): LazyLinear(in_features=0, out_features=100, bias=False)\n",
       "  (W_k): LazyLinear(in_features=0, out_features=100, bias=False)\n",
       "  (W_v): LazyLinear(in_features=0, out_features=100, bias=False)\n",
       "  (W_o): LazyLinear(in_features=0, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "f04cfcd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:49.098746Z",
     "iopub.status.busy": "2023-08-18T07:00:49.097981Z",
     "iopub.status.idle": "2023-08-18T07:00:49.117181Z",
     "shell.execute_reply": "2023-08-18T07:00:49.116256Z"
    },
    "origin_pos": 20,
    "tab": [
     "mxnet"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-15T07:37:39.692716Z",
     "start_time": "2025-10-15T07:37:39.682280Z"
    }
   },
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kv_pairs, valid_lens = 6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kv_pairs, num_hiddens))\n",
    "print(f\"X=queries:\\n{X.shape}\")\n",
    "\n",
    "result = attention(X, Y, Y, valid_lens)\n",
    "print(f\"\\nattention(queries, Y, Y, valid_lens):\\n{result.shape}\")\n",
    "\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=queries:\n",
      "torch.Size([2, 4, 100])\n",
      "\n",
      "attention(queries, Y, Y, valid_lens):\n",
      "torch.Size([2, 4, 100])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "d7fe1dd7",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。\n",
    "* 基于适当的张量操作，可以实现多头注意力的并行计算。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
