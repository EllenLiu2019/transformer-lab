{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6875f27",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 用于预训练BERT的数据集\n",
    ":label:`sec_bert-dataset`\n",
    "\n",
    "为了预训练 :numref:`sec_bert`中实现的BERT模型，我们需要以理想的格式生成数据集，以便于两个预训练任务：遮蔽语言模型和下一句预测。一方面，最初的BERT模型是在两个庞大的图书语料库和英语维基百科（参见 :numref:`subsec_bert_pretraining_tasks`）的合集上预训练的，但它很难吸引这本书的大多数读者。另一方面，现成的预训练BERT模型可能不适合医学等特定领域的应用。因此，在定制的数据集上对BERT进行预训练变得越来越流行。为了方便BERT预训练的演示，我们使用了较小的语料库WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`。\n",
    "\n",
    "与 :numref:`sec_word2vec_data`中用于预训练word2vec的PTB数据集相比，WikiText-2（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；（3）大了一倍以上。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "342b7589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:38.284931Z",
     "iopub.status.busy": "2023-08-18T07:00:38.284353Z",
     "iopub.status.idle": "2023-08-18T07:00:41.113963Z",
     "shell.execute_reply": "2023-08-18T07:00:41.112838Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:31.924641Z",
     "start_time": "2025-10-19T03:43:31.856678Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ],
   "outputs": [],
   "execution_count": 247
  },
  {
   "cell_type": "markdown",
   "id": "691a2248",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "In the WikiText-2 dataset, each line represents a paragraph where space is inserted between any punctuation and its preceding token. Paragraphs with at least two sentences are retained. To split sentences, we only use the period as the delimiter for simplicity. We leave discussions of more complex sentence splitting techniques in the exercises at the end of this section.\n",
    "\n",
    "在 WikiText-2 数据集中，每行代表一个段落，其中任何标点符号与其前面的 token 之间都插入了空格。只保留 至少有两句话的 段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。关于更复杂的句子拆分技术的讨论，我们留到本节末尾的练习中进行。\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.168524Z",
     "start_time": "2025-10-19T03:43:31.930882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "def _read_wiki(data_dir):\n",
    "    # parquet文件名为 wiki.train.tokens.parquet\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens.parquet')\n",
    "\n",
    "    # 使用 pyarrow 直接读取\n",
    "    table = pq.read_table(file_name)\n",
    "    df = table.to_pandas()\n",
    "\n",
    "     # 处理数据\n",
    "    if 'text' in df.columns:\n",
    "        lines = df['text'].tolist()\n",
    "    else:\n",
    "        first_column = df.columns[0]\n",
    "        lines = df[first_column].tolist()\n",
    "\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "paragraphs = _read_wiki(\"../data\")"
   ],
   "id": "b9370b29b779c405",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.177479Z",
     "start_time": "2025-10-19T03:43:32.173695Z"
    }
   },
   "cell_type": "code",
   "source": "len(paragraphs), paragraphs[0], len(paragraphs[0]), paragraphs[0][0], paragraphs[0][1]",
   "id": "d3aecccdbe8e9448",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15496,\n",
       " ['\" don \\'t you wanna stay \" was covered by colton dixon and <unk> <unk> in the eleventh season of american idol',\n",
       "  'natalie finn of e ! gave a mixed review of the pair \\'s performance , writing \" <unk> handled kelly clarkson better than colton played jason aldean on \" don \\'t you wanna stay , \" but she \\'s the country girl , so it made sense',\n",
       "  '\" brian mansfield of usa today felt that the song was out of dixon \\'s comfort zone and a little out of <unk> \\'s range',\n",
       "  'gil kaufman of mtv remarked that the chemistry between the pair was more like cold fusion',\n",
       "  'jennifer still of digital spy said the performance \" isn \\'t anything incredible \" .'],\n",
       " 5,\n",
       " '\" don \\'t you wanna stay \" was covered by colton dixon and <unk> <unk> in the eleventh season of american idol',\n",
       " 'natalie finn of e ! gave a mixed review of the pair \\'s performance , writing \" <unk> handled kelly clarkson better than colton played jason aldean on \" don \\'t you wanna stay , \" but she \\'s the country girl , so it made sense')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 249
  },
  {
   "cell_type": "markdown",
   "id": "f2f5515b",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 为预训练任务定义辅助函数\n",
    "\n",
    "In the following, we begin by implementing helper functions for the two BERT pretraining tasks: next sentence prediction and masked language modeling. These helper functions will be invoked later when transforming the raw text corpus into the dataset of the ideal format to pretrain BERT.\n",
    "\n",
    "在下文中，我们首先为BERT的两个预训练任务实现辅助函数。这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。\n",
    "\n",
    "### 生成下一句预测任务的数据\n",
    "\n",
    "根据 :numref:`subsec_nsp`的描述，`_get_next_sentence`函数生成二分类任务的训练样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "246ca273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.128645Z",
     "iopub.status.busy": "2023-08-18T07:00:41.128375Z",
     "iopub.status.idle": "2023-08-18T07:00:41.133471Z",
     "shell.execute_reply": "2023-08-18T07:00:41.132347Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.184962Z",
     "start_time": "2025-10-19T03:43:32.183078Z"
    }
   },
   "source": [
    "#@save\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # paragraphs 是三重列表的嵌套\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ],
   "outputs": [],
   "execution_count": 250
  },
  {
   "cell_type": "markdown",
   "id": "13b1d432",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "下面的函数通过调用 `_get_next_sentence` 函数从输入 `paragraph` 生成用于下一句预测的训练样本。这里 `paragraph` 是一个列表（每个元素是句子），其中每个句子都是 token 的列表。自变量 `max_len` 指定预训练期间的BERT输入序列的最大长度。\n",
    "\n",
    "[\n",
    "\n",
    "    [token1, token2, token3, ..., tokenN],\n",
    "\n",
    "    [token1, token2, token3, ..., tokenN]\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7686fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.137934Z",
     "iopub.status.busy": "2023-08-18T07:00:41.137439Z",
     "iopub.status.idle": "2023-08-18T07:00:41.143146Z",
     "shell.execute_reply": "2023-08-18T07:00:41.142265Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.192578Z",
     "start_time": "2025-10-19T03:43:32.190069Z"
    }
   },
   "source": [
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len, verbose=False):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        if verbose:\n",
    "            print(f'---')\n",
    "            print(f'paragraph: {paragraph}\\n')\n",
    "            print(f'tokens_a: {tokens_a}, tokens_b: {tokens_b}, is_next: {is_next}')\n",
    "\n",
    "        # BERT 输入序列长度控制\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "        if verbose:\n",
    "            print(f'---')\n",
    "            print(f'nsp_data_from_paragraph: {nsp_data_from_paragraph}')\n",
    "    return nsp_data_from_paragraph"
   ],
   "outputs": [],
   "execution_count": 251
  },
  {
   "cell_type": "markdown",
   "id": "86277b80",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "### 生成遮蔽语言模型任务的数据\n",
    ":label:`subsec_prepare_mlm_data`\n",
    "\n",
    "In order to generate training examples for the masked language modeling task from a BERT input sequence, we define the following _replace_mlm_tokens function.\n",
    "\n",
    "In its inputs, tokens is a list of tokens representing a BERT input sequence, candidate_pred_positions is a list of token indices of the BERT input sequence excluding those of special tokens (special tokens are not predicted in the masked language modeling task), and num_mlm_preds indicates the number of predictions (recall 15% random tokens to predict).\n",
    "\n",
    "Following the definition of the masked language modeling task in Section 15.8.5.1, at each `prediction position`, the input may be replaced by a special “<mask>” token or a random token, or remain unchanged. In the end, the function returns\n",
    "* the input tokens after possible replacement\n",
    "* the token indices where predictions take place\n",
    "* labels for these predictions\n",
    "\n",
    "为了从 BERT 输入序列生成遮蔽语言模型的训练样本，我们定义了以下 `_replace_mlm_tokens` 函数。\n",
    "\n",
    "在其输入中，`tokens` 是表示 BERT 输入序列的词元的列表，`candidate_pred_positions` 是不包括特殊词元的 BERT 输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测），以及 `num_mlm_preds` 指示预测的数量（选择 15% 要预测的随机词元）。\n",
    "\n",
    "在 :numref: `subsec_mlm` 中定义遮蔽语言模型任务之后，在每个 `预测位置` ，输入可以由特殊的 “掩码” 词元或随机词元替换，或者保持不变。最后，该函数返回\n",
    "* 可能替换后的输入词元\n",
    "* 发生预测的词元索引\n",
    "* 这些预测的标签\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e3de2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.147428Z",
     "iopub.status.busy": "2023-08-18T07:00:41.146946Z",
     "iopub.status.idle": "2023-08-18T07:00:41.155481Z",
     "shell.execute_reply": "2023-08-18T07:00:41.154569Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.197447Z",
     "start_time": "2025-10-19T03:43:32.194769Z"
    }
   },
   "source": [
    "#@save\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
    "                        vocab):\n",
    "    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的 “<mask>” 或随机词元\n",
    "\n",
    "    # mlm_input_tokens 是一个列表，其中可能包含：\n",
    "    # “<mask>” 或 随机词元 或 原始词元\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "\n",
    "    # pred_positions_and_labels 是一个列表，其中包含（除外特殊词元）：\n",
    "    # 词元索引 和 原始词元\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ],
   "outputs": [],
   "execution_count": 252
  },
  {
   "cell_type": "markdown",
   "id": "81ce2383",
   "metadata": {
    "origin_pos": 12
   },
   "source": "通过调用前述的`_replace_mlm_tokens`函数，以下函数将 BERT 输入序列（`tokens`）作为输入，并返回输入词元的索引（在 :numref:`subsec_mlm`中描述的可能的词元替换之后）、发生预测的词元索引以及这些预测的标签索引。\n"
  },
  {
   "cell_type": "code",
   "id": "841a4650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.160061Z",
     "iopub.status.busy": "2023-08-18T07:00:41.159300Z",
     "iopub.status.idle": "2023-08-18T07:00:41.165820Z",
     "shell.execute_reply": "2023-08-18T07:00:41.164855Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.202605Z",
     "start_time": "2025-10-19T03:43:32.199852Z"
    }
   },
   "source": [
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab, verbose=False):\n",
    "    # tokens是一个字符串列表\n",
    "\n",
    "    # 获取被 mask 的 tokens 的索引\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "\n",
    "    # 获取被 mask 的 tokens 的数量，tokens 总数的 15%\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    if verbose:\n",
    "        print(f\"\\ntokens 总数的 15%:\\n {num_mlm_preds}\")\n",
    "        print(f\"\\ntokens:\\n {tokens}\")\n",
    "        print(f\"\\ncandidate_pred_positions:\\n {candidate_pred_positions}\")\n",
    "        print(f\"\\nvocab:\\n{vocab}\")\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "\n",
    "    # 按 位置索引 排序预测位置和标签\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    if verbose:\n",
    "        print(f\"\\nmlm_input_tokens:\\n{mlm_input_tokens}\")\n",
    "        print(f\"\\npred_positions_and_labels:\\n{pred_positions_and_labels}\")\n",
    "        print(f\"\\npred_positions:\\n{pred_positions}\")\n",
    "        print(f\"\\nmlm_pred_labels:\\n{mlm_pred_labels}\")\n",
    "        print(f\"\\nvocab[mlm_input_tokens]:\\n{vocab[mlm_input_tokens]}\")\n",
    "        print(f\"\\nvocab[mlm_pred_labels]:\\n{vocab[mlm_pred_labels]}\")\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ],
   "outputs": [],
   "execution_count": 253
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这里 vocab[mlm_input_tokens] 的作用是：\n",
    "* mlm_input_tokens 是一个字符串词元列表，如 ['\\<cls\\>', 'i', '\\<mask\\>', 'bert', '\\<sep\\>']\n",
    "* 通过 vocab[mlm_input_tokens] 将每个词元转换为其在词汇表中的索引\n",
    "* 返回结果是一个索引列表，如 [2, 156, 234, 5, 3]"
   ],
   "id": "8e3136c9ccebcad2"
  },
  {
   "cell_type": "markdown",
   "id": "396550b1",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 将文本转换为预训练数据集\n",
    "\n",
    "现在我们几乎准备好为BERT预训练定制一个 `Dataset` 类。\n",
    "\n",
    "append the special “\\<pad\\>” tokens to the inputs.\n",
    "\n",
    "在此之前，我们仍然需要定义辅助函数 `_pad_bert_inputs` 来将特殊的 “&lt;pad&gt;” 词元附加到输入。\n",
    "它的参数`examples`包含来自两个预训练任务的辅助函数 `_get_nsp_data_from_paragraph` 和 `_get_mlm_data_from_tokens` 的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6552099b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.170203Z",
     "iopub.status.busy": "2023-08-18T07:00:41.169578Z",
     "iopub.status.idle": "2023-08-18T07:00:41.180126Z",
     "shell.execute_reply": "2023-08-18T07:00:41.179219Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.207618Z",
     "start_time": "2025-10-19T03:43:32.205054Z"
    }
   },
   "source": [
    "#@save\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    # 对BERT模型的输入数据进行填充，确保同一批次中的所有样本具有相同的长度。\n",
    "\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)),\n",
    "                                          dtype=torch.long))\n",
    "\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)),\n",
    "                                         dtype=torch.long))\n",
    "\n",
    "        # valid_lens不包括'<pad>'的计数\n",
    "        valid_lens.append(torch.tensor(len(token_ids),\n",
    "                                       dtype=torch.float32))\n",
    "\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                                               dtype=torch.long))\n",
    "\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                                            dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)),\n",
    "                                           dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next,\n",
    "                                       dtype=torch.long))\n",
    "\n",
    "    return (all_token_ids, all_segments,\n",
    "            valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ],
   "outputs": [],
   "execution_count": 254
  },
  {
   "cell_type": "markdown",
   "id": "d4e8a88c",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "将用于生成两个预训练任务的训练样本的辅助函数和用于填充输入的辅助函数放在一起，我们定义以下`_WikiTextDataset`类为用于预训练BERT的WikiText-2数据集。通过实现`__getitem__ `函数，我们可以任意访问WikiText-2语料库的一对句子生成的预训练样本（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "最初的BERT模型使用词表大小为30000的WordPiece嵌入 :cite:`Wu.Schuster.Chen.ea.2016`。WordPiece的词元化方法是对 :numref:`subsec_Byte_Pair_Encoding`中原有的字节对编码算法稍作修改。为简单起见，我们使用`d2l.tokenize`函数进行词元化。出现次数少于5次的不频繁词元将被过滤掉。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4d049c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.184551Z",
     "iopub.status.busy": "2023-08-18T07:00:41.183947Z",
     "iopub.status.idle": "2023-08-18T07:00:41.192539Z",
     "shell.execute_reply": "2023-08-18T07:00:41.191426Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.213449Z",
     "start_time": "2025-10-19T03:43:32.210221Z"
    }
   },
   "source": [
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len, verbose = False):\n",
    "\n",
    "        # 输入 paragraphs[i] 是代表段落的句子字符串列表；\n",
    "        # 而输出 paragraphs[i] 是代表段落的句子列表，其中每个句子都是词元列表\n",
    "        print(f\"before tokenize, paragraphs:\\n{paragraphs}\")\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        print(f\"after tokenize, paragraphs:\\n{paragraphs}\")\n",
    "\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        print(f'\\nsentences：\\n{sentences}')\n",
    "        self.vocab = d2l.Vocab(tokens=sentences, min_freq=5,\n",
    "                               reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "\n",
    "        # 获取下一句子预测任务的数据\n",
    "        examples = []\n",
    "        print(f\"\\n len(paragraphs): {len(paragraphs)}\")\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            #if i == 0:\n",
    "                #verbose = True\n",
    "            #else:\n",
    "                #verbose = False\n",
    "            examples.extend(\n",
    "                _get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len, verbose=verbose))\n",
    "\n",
    "        # 获取 遮蔽语言模型任务 的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab, verbose = verbose) + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        print(f'\\nlen of examples\\n: {len(examples)}')\n",
    "        print(f'\\nexamples[:1]\\n: {examples[:1]}')\n",
    "\n",
    "        # 填充输入\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ],
   "outputs": [],
   "execution_count": 255
  },
  {
   "cell_type": "markdown",
   "id": "0ede31c0",
   "metadata": {
    "origin_pos": 22
   },
   "source": "通过使用 `_read_wiki` 函数和 `_WikiTextDataset` 类，我们定义了下面的 `load_data_wiki` 来下载并生成WikiText-2数据集，并从中生成预训练样本。\n"
  },
  {
   "cell_type": "code",
   "id": "9b484a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.197261Z",
     "iopub.status.busy": "2023-08-18T07:00:41.196591Z",
     "iopub.status.idle": "2023-08-18T07:00:41.202074Z",
     "shell.execute_reply": "2023-08-18T07:00:41.201154Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.217526Z",
     "start_time": "2025-10-19T03:43:32.215893Z"
    }
   },
   "source": [
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = 0 # d2l.get_dataloader_workers()\n",
    "    #data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    data_dir = \"../data\"\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs[:1][:2], max_len, verbose=False)\n",
    "    print(train_set[0])\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                        shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ],
   "outputs": [],
   "execution_count": 256
  },
  {
   "cell_type": "markdown",
   "id": "74b59eb9",
   "metadata": {
    "origin_pos": 26
   },
   "source": "将批量大小设置为 512，将 BERT 输入序列的最大长度设置为 64，我们打印出小批量的 BERT 预训练样本的形状。注意，在每个 BERT 输入序列中，为遮蔽语言模型任务预测 $10$（$64 \\times 0.15$）个位置。\n"
  },
  {
   "cell_type": "code",
   "id": "f1a8e103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.206083Z",
     "iopub.status.busy": "2023-08-18T07:00:41.205815Z",
     "iopub.status.idle": "2023-08-18T07:00:52.152614Z",
     "shell.execute_reply": "2023-08-18T07:00:52.151321Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.376928Z",
     "start_time": "2025-10-19T03:43:32.226916Z"
    }
   },
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tokenize, paragraphs:\n",
      "[['in 1928 , m @-@ 111 was assigned to a route connecting m @-@ 13 ( later signed as us 23 for a time ) north of bay city to bay city state park on saginaw bay', 'the original route consisted of what is today euclid avenue', 'in the early 1930s , a return leg towards bay city was added to the east of the original route along what is now state park road , giving the route an upside @-@ down @-@ u <unk> 1933 , the western leg along euclid avenue from midland road to beaver road was designated as m @-@ 47', 'in 1938 , all of m @-@ 111 was re @-@ designated as m @-@ 47 — thus making m @-@ 47 double back to bay city .']]\n",
      "after tokenize, paragraphs:\n",
      "[[['in', '1928', ',', 'm', '@-@', '111', 'was', 'assigned', 'to', 'a', 'route', 'connecting', 'm', '@-@', '13', '(', 'later', 'signed', 'as', 'us', '23', 'for', 'a', 'time', ')', 'north', 'of', 'bay', 'city', 'to', 'bay', 'city', 'state', 'park', 'on', 'saginaw', 'bay'], ['the', 'original', 'route', 'consisted', 'of', 'what', 'is', 'today', 'euclid', 'avenue'], ['in', 'the', 'early', '1930s', ',', 'a', 'return', 'leg', 'towards', 'bay', 'city', 'was', 'added', 'to', 'the', 'east', 'of', 'the', 'original', 'route', 'along', 'what', 'is', 'now', 'state', 'park', 'road', ',', 'giving', 'the', 'route', 'an', 'upside', '@-@', 'down', '@-@', 'u', '<unk>', '1933', ',', 'the', 'western', 'leg', 'along', 'euclid', 'avenue', 'from', 'midland', 'road', 'to', 'beaver', 'road', 'was', 'designated', 'as', 'm', '@-@', '47'], ['in', '1938', ',', 'all', 'of', 'm', '@-@', '111', 'was', 're', '@-@', 'designated', 'as', 'm', '@-@', '47', '—', 'thus', 'making', 'm', '@-@', '47', 'double', 'back', 'to', 'bay', 'city', '.']]]\n",
      "\n",
      "sentences：\n",
      "[['in', '1928', ',', 'm', '@-@', '111', 'was', 'assigned', 'to', 'a', 'route', 'connecting', 'm', '@-@', '13', '(', 'later', 'signed', 'as', 'us', '23', 'for', 'a', 'time', ')', 'north', 'of', 'bay', 'city', 'to', 'bay', 'city', 'state', 'park', 'on', 'saginaw', 'bay'], ['the', 'original', 'route', 'consisted', 'of', 'what', 'is', 'today', 'euclid', 'avenue'], ['in', 'the', 'early', '1930s', ',', 'a', 'return', 'leg', 'towards', 'bay', 'city', 'was', 'added', 'to', 'the', 'east', 'of', 'the', 'original', 'route', 'along', 'what', 'is', 'now', 'state', 'park', 'road', ',', 'giving', 'the', 'route', 'an', 'upside', '@-@', 'down', '@-@', 'u', '<unk>', '1933', ',', 'the', 'western', 'leg', 'along', 'euclid', 'avenue', 'from', 'midland', 'road', 'to', 'beaver', 'road', 'was', 'designated', 'as', 'm', '@-@', '47'], ['in', '1938', ',', 'all', 'of', 'm', '@-@', '111', 'was', 're', '@-@', 'designated', 'as', 'm', '@-@', '47', '—', 'thus', 'making', 'm', '@-@', '47', 'double', 'back', 'to', 'bay', 'city', '.']]\n",
      "\n",
      " len(paragraphs): 1\n",
      "\n",
      "len of examples\n",
      ": 2\n",
      "\n",
      "examples[:1]\n",
      ": [([1, 5, 5, 0, 8, 6, 5, 2, 5, 10, 2, 5, 5, 8, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 7, 5, 10, 7, 5, 5, 5, 5, 5, 7, 4, 9, 5, 5, 2, 5, 5, 5, 5, 2, 5, 4], [4, 7, 10, 16, 21, 27, 42, 47], [8, 5, 5, 5, 5, 5, 5, 5], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)]\n",
      "(tensor([ 1,  5,  5,  0,  8,  6,  5,  2,  5, 10,  2,  5,  5,  8,  6,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  2,  7,  5, 10,  7,  5,  5,  5,  5,\n",
      "         5,  7,  4,  9,  5,  5,  2,  5,  5,  5,  5,  2,  5,  4,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(50.), tensor([ 4,  7, 10, 16, 21, 27, 42, 47,  0,  0]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 0.]), tensor([8, 5, 5, 5, 5, 5, 5, 5, 0, 0]), tensor(1))\n",
      "torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2]) torch.Size([2, 10]) torch.Size([2, 10]) torch.Size([2, 10]) torch.Size([2])\n"
     ]
    }
   ],
   "execution_count": 257
  },
  {
   "cell_type": "markdown",
   "id": "c8b78dd7",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "最后，我们来看一下词量。即使在过滤掉不频繁的词元之后，它仍然比PTB数据集的大两倍以上。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "47b86684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:52.159404Z",
     "iopub.status.busy": "2023-08-18T07:00:52.158958Z",
     "iopub.status.idle": "2023-08-18T07:00:52.169643Z",
     "shell.execute_reply": "2023-08-18T07:00:52.168438Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2025-10-19T03:43:32.385093Z",
     "start_time": "2025-10-19T03:43:32.382043Z"
    }
   },
   "source": "len(vocab)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 258
  },
  {
   "cell_type": "markdown",
   "id": "081adbe2",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 与PTB数据集相比，WikiText-2数据集保留了原来的标点符号、大小写和数字，并且比PTB数据集大了两倍多。\n",
    "* 我们可以任意访问从WikiText-2语料库中的一对句子生成的预训练（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为简单起见，句号用作拆分句子的唯一分隔符。尝试其他的句子拆分技术，比如Spacy和NLTK。以NLTK为例，需要先安装NLTK：`pip install nltk`。在代码中先`import nltk`。然后下载Punkt语句词元分析器：`nltk.download('punkt')`。要拆分句子，比如`sentences = 'This is great ! Why not ?'`，调用`nltk.tokenize.sent_tokenize(sentences)`将返回两个句子字符串的列表：`['This is great !', 'Why not ?']`。\n",
    "1. 如果我们不过滤出一些不常见的词元，词量会有多大？\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
